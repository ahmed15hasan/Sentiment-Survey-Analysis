# -*- coding: utf-8 -*-
"""RAGforExecl V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdozYhG9SjO9MF4vsapfuntXwpRDuTUK
"""

!pip install torch transformers langchain langchain-community

!pip install langchain_huggingface

!pip install chromadb

!pip install langchain_community

from huggingface_hub import login
login("")

"""# New Code"""

!pip install docx2txt

from langchain_huggingface import HuggingFacePipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd
#""

model_name="mistralai/Mistral-7B-Instruct-v0.2"
#model_name="meta-llama/Llama-3.2-1B"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token="")
model= AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float32,device_map="auto",use_auth_token="")  # Change to float32 for CPU

import json
import re

from langchain.docstore.document import Document

class RAGPDFAnalyzer:
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.2",model=model,tokenizer=tokenizer):  # Use a smaller model for compatibility
        self.tokenizer = tokenizer

        self.model = model
        self.text_pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_length=1024,
            temperature=0.5,
            top_p=0.95,
            repetition_penalty=1.15,
            torch_dtype=torch.bfloat16,
            device_map="auto")
        self.llm = HuggingFacePipeline(pipeline=self.text_pipeline)  # Initialize the HuggingFacePipeline here
        self.db = None

    def load_excel_to_db(self, excel_path):
        """
        Load an Excel file with question-answer pairs into the vector database.

        Parameters:
            excel_path (str): Path to the Excel file containing the questions and answers.
        """
        try:
            # Load the Excel file into a DataFrame
            df = pd.read_excel(excel_path)
        except Exception as e:
            raise ValueError(f"Error reading the Excel file: {e}")

        # Check if required columns exist
        if not all(col in df.columns for col in ['Question', 'Answer']):
            raise ValueError("Excel sheet must contain 'Question' and 'Answer' columns.")

        # Combine questions and answers into documents
        documents = []
        for _, row in df.iterrows():
          content = f"Question: {row['Question']}\nAnswer: {row['Answer']}"
          documents.append(Document(page_content=content))
        return documents

    def load_db(self, file_path):
        if file_path.lower().endswith('.xls') or file_path.lower().endswith('.xlsx'):
            document_text = self.load_excel_to_db(file_path)
        else:
            raise ValueError("Unsupported file type. Supported types are: '.xls' & '.xlsx'")

        # Split the text into manageable chunks
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
        self.texts = self.text_splitter.split_documents(document_text)
        # Correct device setup
        device = "cuda" if torch.cuda.is_available() else "cpu"
        # Generate embeddings for the texts
        self.embeddings = HuggingFaceEmbeddings(model_name="hkunlp/instructor-xl", model_kwargs={"device": device})  # Set to cpu for compatibility
        self.db = Chroma.from_documents(self.texts, self.embeddings)

    def generate_response(self, query):
        prompt = PromptTemplate(template="""Analyze the survey's question and answer. Provide a sentiment analysis in JSON format as shown below and sort it:

  question: question,
  answer: answer,
  sentiment: positive|negative|neutral

Analyze the sentiment of the response and label it as 'positive,' 'negative,' or 'neutral.'
context: {context}
user: {question}
chatbot:""", input_variables=["context", "question"])


        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.db.as_retriever(search_kwargs={"k": 2}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt},
        )

        query_result = qa_chain(query)
        response_content = query_result['result']
        answer_prefix = "chatbot: "
        answer_start_index = response_content.find(answer_prefix)
        if answer_start_index != -1:
            answer = response_content[answer_start_index + len(answer_prefix):].strip()
            print(answer)
            return answer
        else:
            print("No answer found in the response.")
            return response_content

    def analyze_document(self, file_path):
        self.load_db(file_path)
        sections_queries = {
            "questions": "Analysis the Document and give the Question , Response and sentiment of that response in json "
            "summery"
        }
        project_info = {}

        for section, query in sections_queries.items():
            response = self.generate_response(query)
            print(f"Response for {section}: {response}")
            # Free all unused cached memory
            torch.cuda.empty_cache()

            # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues
            torch.cuda.reset_max_memory_allocated()
            torch.cuda.reset_max_memory_cached()
            # Free all unused cached memory
            torch.cuda.empty_cache()

            # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues
            torch.cuda.reset_max_memory_allocated()
            torch.cuda.reset_max_memory_cached()
            # Free all unused cached memory
            torch.cuda.empty_cache()

            # Optionally, reset the CUDA memory allocator to prevent potential fragmentation issues
            torch.cuda.reset_max_memory_allocated()
            torch.cuda.reset_max_memory_cached()




    def extract_json_from_text(self,text):
        """
        Extract the first JSON structure from the text using regex.

        Parameters:
            text (str): The text containing JSON.

        Returns:
            dict: The extracted JSON, or None if no JSON is found.
        """
        try:
            # Define a simple regex to extract JSON object from the text
            json_str = re.search(r'(\{.*\})', text, re.DOTALL)  # matches JSON-like content
            if json_str:
                return json.loads(json_str.group(1))  # Parse the matched JSON string
            else:
                return None
        except json.JSONDecodeError:
            return None

    def extract_sentiment_from_json(self,json_obj):
        """
        Extract the sentiment label from a JSON object.

        Parameters:
            json_obj (dict): The JSON object.

        Returns:
            str: The sentiment label (e.g., 'POSITIVE', 'NEGATIVE'), or 'UNKNOWN' if not found.
        """
        try:
            # Check if the 'sentiment' key exists and get the value
            sentiment_data = json_obj.get('sentiment')

            # If 'sentiment' is a string, return it directly
            if isinstance(sentiment_data, str):
                return sentiment_data

            # If 'sentiment' is a dictionary, check if it has a 'label' key and return its value
            elif isinstance(sentiment_data, dict):
                # Check if 'label' exists in the nested sentiment dictionary
                label = sentiment_data.get('label', 'UNKNOWN')
                if label:
                    return label
                else:
                    # If 'label' is not found, check for the 'sentiment' in the dictionary itself
                    return sentiment_data.get('sentiment')
                                # If 'sentiment' is a string, return it directly
            else:
                sentiment_data = json_obj.get('chatbot')
                if isinstance(sentiment_data, dict):
                    # Check if 'label' exists in the nested sentiment dictionary
                    label = sentiment_data.get('sentiment', 'UNKNOWN')
                    if label:
                        return label
                    else:
                        # If 'label' is not found, check for the 'sentiment' in the dictionary itself
                        return 'UNKNOWN'
            # If sentiment is not found or is neither string nor dictionary, return 'UNKNOWN'
            #return 'UNKNOWN'
        except Exception as e:
            print(f"Error extracting sentiment: {e}")
            return 'ERROR'

    def analyze_sentiments_from_excel(self, excel_path, start_index=0, end_index=None):
        """
        Load questions from an Excel file, analyze sentiments dynamically,
        and save the sentiment back to the Excel file for each row within a specific index range.

        Parameters:
            excel_path (str): Path to the Excel file containing the questions and answers.
            start_index (int): The starting index of the rows to process (default is 0).
            end_index (int): The ending index (exclusive) of the rows to process. If None, process all rows.

        Returns:
            str: Path to the updated Excel file with sentiments saved.
        """
        try:
            # Load the Excel file into a DataFrame
            df = pd.read_excel(excel_path)
        except Exception as e:
            raise ValueError(f"Error reading the Excel file: {e}")

        # Check if required columns exist
        if not all(col in df.columns for col in ['Question', 'Answer']):
            raise ValueError("Excel sheet must contain 'Question' and 'Answer' columns.")
        self.load_db(excel_path)
        # Add a column for sentiments if not already present
        if 'Predicted Sentiment' not in df.columns:
            df['Predicted Sentiment'] = ""

        # Define the range of rows to process
        end_index = len(df) if end_index is None else min(end_index, len(df))

        # Iterate through the specified index range and analyze sentiment
        for index in range(start_index, end_index):
            row = df.iloc[index]
            question = f"Analyze the Document and give the Question, Response and sentiment of that response in json of that question: {row['Question']}"
            response_json_str = self.generate_response(question)

            # Extract JSON from the response text
            json_obj = self.extract_json_from_text(response_json_str)

            # Extract sentiment or fallback to 'UNKNOWN'
            sentiment_label = self.extract_sentiment_from_json(json_obj) if json_obj else 'UNKNOWN'

            # Update the 'Predicted Sentiment' column for the current row
            df.at[index, 'Predicted Sentiment'] = sentiment_label

            # Save the updated DataFrame to the Excel file after processing each row
            try:
                df.to_excel(excel_path.replace('.xls', '.xlsx'), index=False)
            except Exception as e:
                raise ValueError(f"Error saving the Excel file: {e}")

            print(f"Processed row {index + 1}/{len(df)}: Sentiment - {sentiment_label}")

        print(f"Sentiments successfully saved to {excel_path}")
        return excel_path



# Step 1: Initialize the RAG Analyzer
analyzer = RAGPDFAnalyzer()

# Step 2: Analyze Sentiments from an Excel File
result = analyzer.analyze_sentiments_from_excel("/content/Sample_Survey_Excel.xls", start_index=0, end_index=None)

# Step 3: Output Results
# print("Sentiment Analysis Results:")
# for question, sentiment in result.items():
#     print(f"Question: {question}")
#     print(f"Sentiment: {sentiment}")
#     print("-" * 40)

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from joblib import Parallel, delayed
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.metrics import r2_score
from scipy import stats
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from tensorflow.keras.utils import to_categorical

from sklearn import preprocessing
def mlresults(y_test,pred):
  accuracy = metrics.accuracy_score(y_test,pred)
  confusion_matrix = metrics.confusion_matrix(y_test,pred)
  classification = metrics.classification_report(y_test,pred)
  le = preprocessing.LabelEncoder()
  target=le.fit_transform(y_test)
  predicted = le.fit_transform(pred)
  accuracy = metrics.accuracy_score(target, predicted)
  mse=mean_squared_error(target, predicted)
  rs=r2_score(target, predicted)
  # print('============================== {} Model Test Results =============================='.format(modelname))
  # print()
  # print ("Model Accuracy:" "\n", accuracy)
  # print()
  # print("Confusion matrix:" "\n", confusion_matrix)
  # print()
  # print("Classification report:" "\n", classification)
  # print()
  # print ("Model Accuracy:" "\n", accuracy)
  # print("Mean squared error: ", mse)
  # print('Variance score: %.2f' % rs)

  return accuracy,mse,rs

import pandas as pd

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing

def mlresults(y_test, pred):
    # Encode the labels if they are categorical
    le = preprocessing.LabelEncoder()
    target = le.fit_transform(y_test)
    predicted = le.fit_transform(pred)

    # Calculate the accuracy, MSE, and R2 score
    accuracy = accuracy_score(target, predicted)
    mse = mean_squared_error(target, predicted)
    rs = r2_score(target, predicted)

    # Confusion matrix
    cm = confusion_matrix(target, predicted)

    # Plot confusion matrix as a heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # Optional: Print classification report
    classification = classification_report(target, predicted)
    print("Classification Report:\n", classification)

    # Return the results
    return accuracy, mse, rs

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score

# Function to convert categorical data to numeric if necessary
def convert_to_numeric(series):
    if series.dtype == 'object':  # If it's categorical (strings)
        le = LabelEncoder()
        return le.fit_transform(series)
    return series  # Return as is if already numeric

# Read the Excel file
file_path = "/content/Sample_Survey_Excel.xlsx" # Change this to the path of your file
results = pd.read_excel(file_path)
results= results
# Assuming 'Targeted Sentiment' is your target column, adjust the column name if needed
target = convert_to_numeric(results['Sentiment'])

# Assuming models start from column index 4 onwards, adjust this if needed
predicted = convert_to_numeric(results['Predicted Sentiment'])
accuracy, mse, rs = mlresults(target, predicted)

modelnames =['Predicted Sentiment']
# DataFrame to store the results
resultscore = pd.DataFrame(columns=['Model', 'Accuracy', 'Mean Squared Error', 'Root2 Score'])

# Add the result to the DataFrame
resultscore.loc[0] = ['Mistral RAG', accuracy, mse, rs]

# Print or save the result
print(resultscore)
# Optionally save to a new Excel file
resultscore.to_excel("model_performance_results.xlsx", index=False)

resultscore